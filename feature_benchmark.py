# feature_benchmark.py
import sys
from datetime import datetime, timedelta

import numpy as np
import pandas as pd
from sklearn.metrics import roc_auc_score, log_loss

from config import Config
from data_loader import DataLoader
from indicators import FeatureEngineer
from model_engine import MLEngine


def evaluate_feature_set(name: str, feature_cols: list[str], df_source: pd.DataFrame):
    print("\n" + "=" * 60)
    print(f"üß™ FEATURE SET: {name}")
    print("=" * 60)

    # 1) –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ–∏—á–∏
    cols = [c for c in feature_cols if c in df_source.columns]
    missing = [c for c in feature_cols if c not in df_source.columns]
    if missing:
        print(f"   ‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω—ã –∫–æ–ª–æ–Ω–∫–∏ (–Ω–µ—Ç –≤ df): {missing}")
    if len(cols) < 3:
        print("   ‚ùå –°–ª–∏—à–∫–æ–º –º–∞–ª–æ —Ñ–∏—á –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏, –ø—Ä–æ–ø—É—Å–∫.")
        return None

    print(f"   ‚úÖ –ë—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å {len(cols)} —Ñ–∏—á: {cols}")

    # 2) –ß–∏—Å—Ç–∏–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –ø–æ–¥ —ç—Ç–æ—Ç –Ω–∞–±–æ—Ä
    needed_cols = cols + ["target", "fwd_return"]
    df = df_source.dropna(subset=needed_cols).copy()
    if len(df) < 500:
        print(f"   ‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {len(df)}")
        return None

    # 3) –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å (–≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–ø–ª–∏—Ç –≤–Ω—É—Ç—Ä–∏ MLEngine)
    engine = MLEngine(model_dir=None, regime_preset="auto")
    engine.train(df, cols, target_col="target")

    # 4) –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ –≤—Å–µ–π –≤—ã–±–æ—Ä–∫–µ –∏ –≤—ã–¥–µ–ª–µ–Ω–∏–µ OOS-—Ö–≤–æ—Å—Ç–∞
    probs, regimes = engine.predict_batch(df, cols)
    if probs is None:
        print("   ‚ùå predict_batch –≤–µ—Ä–Ω—É–ª None.")
        return None

    n = len(df)
    test_start = int(n * 0.85)
    y_true_full = df["target"].values
    fwd_full = df["fwd_return"].values

    y_test = y_true_full[test_start:]
    p_test = probs[test_start:]
    fwd_test = fwd_full[test_start:]

    # 5) –§–∏–ª—å—Ç—Ä –≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Ç—Ä–æ–∫
    valid_mask = (
        (y_test >= 0)
        & (y_test <= 2)
        & np.isfinite(p_test).all(axis=1)
        & np.isfinite(fwd_test)
    )

    y_val = y_test[valid_mask]
    p_val = p_test[valid_mask]
    fwd_val = fwd_test[valid_mask]

    if len(y_val) < 100:
        print(f"   ‚ùå –ú–∞–ª–æ —Ç–æ—á–µ–∫ –≤ —Ç–µ—Å—Ç–µ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(y_val)}")
        return None

    # –ù–æ—Ä–º–∏—Ä—É–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ—Å—Ç—Ä–æ—á–Ω–æ
    row_sums = p_val.sum(axis=1, keepdims=True)
    row_sums[row_sums == 0] = 1.0
    p_val = p_val / row_sums

    # 6) AUC / logloss
    classes_present = np.unique(y_val)
    if len(classes_present) < 2:
        auc = np.nan
        print("   ‚ö†Ô∏è –í —Ç–µ—Å—Ç–µ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å ‚Äî AUC –Ω–µ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è.")
    else:
        try:
            auc = roc_auc_score(y_val, p_val, multi_class="ovr")
        except Exception as e:
            print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á—ë—Ç–µ AUC: {e}")
            auc = np.nan

    try:
        ll = log_loss(y_val, p_val, labels=[0, 1, 2])
    except Exception as e:
        print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á—ë—Ç–µ logloss: {e}")
        ll = np.nan

    # 7) –ü—Ä–æ—Å—Ç–µ–π—à–∏–π Sharpe –ø–æ —Å–∏–≥–Ω–∞–ª–∞–º –Ω–∞ 1-–±–∞—Ä–Ω–æ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç–µ
    p_neutral = p_val[:, 0]
    p_long = p_val[:, 1]
    p_short = p_val[:, 2]

    min_edge = getattr(Config, "MIN_EDGE", 0.15)
    edge_long = p_long - p_neutral
    edge_short = p_short - p_neutral

    long_mask = (p_long > p_short) & (edge_long > min_edge)
    short_mask = (p_short > p_long) & (edge_short > min_edge)

    direction = np.zeros_like(p_long)
    direction[long_mask] = 1.0
    direction[short_mask] = -1.0

    # –î–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –Ω–∞ 1 –±–∞—Ä –≤–ø–µ—Ä—ë–¥ * –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
    signal_ret = fwd_val * direction
    signal_ret = signal_ret[direction != 0]

    trades_count = len(signal_ret)
    if trades_count > 1 and signal_ret.std() > 1e-8:
        # –£—Å–ª–æ–≤–Ω–æ-–≥–æ–¥–æ–≤–æ–π Sharpe, –ø—Ä–æ—Å—Ç–æ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ–∂–¥—É –Ω–∞–±–æ—Ä–∞–º–∏
        sharpe = (signal_ret.mean() / signal_ret.std()) * np.sqrt(365)
    else:
        sharpe = 0.0

    print(f"   üßæ OOS samples: {len(y_val)} | Signals: {trades_count}")
    print(f"   üéØ AUC:      {auc:.4f}" if not np.isnan(auc) else "   üéØ AUC:      n/a")
    print(f"   üìâ LogLoss:  {ll:.4f}" if not np.isnan(ll) else "   üìâ LogLoss:  n/a")
    print(f"   ‚ô†Ô∏è Sharpe:   {sharpe:.3f}")

    return {
        "name": name,
        "n_features": len(cols),
        "n_oos": int(len(y_val)),
        "signals": int(trades_count),
        "auc": float(auc) if not np.isnan(auc) else np.nan,
        "logloss": float(ll) if not np.isnan(ll) else np.nan,
        "sharpe": float(sharpe),
    }


def main():
    print("üß™ FEATURE LAB v1.0 ‚Äî —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞–±–æ—Ä–æ–≤ —Ñ–∏—á –Ω–∞ –æ–¥–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ")
    print(f"   –õ–∏–¥–µ—Ä: {Config.LEADER_SYMBOL}, –¢–§: {Config.TIMEFRAME_LTF}")

    end = datetime.now()
    start = end - timedelta(days=1800)

    print(f"   –ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö: {start} -> {end}")

    data = DataLoader.get_portfolio_data(
        [Config.LEADER_SYMBOL],
        Config.LEADER_SYMBOL,
        start,
        end,
        Config.TIMEFRAME_LTF,
        Config.TIMEFRAME_HTF,
    )

    if not data or Config.LEADER_SYMBOL not in data:
        print("‚ùå DataLoader –Ω–µ –≤–µ—Ä–Ω—É–ª –¥–∞–Ω–Ω—ã—Ö –ø–æ –ª–∏–¥–µ—Ä—É, –≤—ã—Ö–æ–¥.")
        sys.exit(1)

    df_raw = data[Config.LEADER_SYMBOL]
    if df_raw is None or df_raw.empty:
        print("‚ùå –ü—É—Å—Ç–æ–π DataFrame –ø–æ –ª–∏–¥–µ—Ä—É.")
        sys.exit(1)

    # –§–∏—á–∏ + —Ç–∞—Ä–≥–µ—Ç
    print("   üõ† –†–∞—Å—á—ë—Ç —Ñ–∏—á–µ–π –∏ —Ç–∞—Ä–≥–µ—Ç–∞...")
    df_feat = FeatureEngineer.add_features(df_raw.copy())
    df_lbl = FeatureEngineer.label_data(df_feat, Config.LOOK_AHEAD, Config.RR_RATIO)

    # –ü—Ä–æ—Å—Ç–∞—è 1-–±–∞—Ä–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –≤–ø–µ—Ä—ë–¥, —á—Ç–æ–±—ã –æ—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Å–∏–≥–Ω–∞–ª–æ–≤
    df_lbl["fwd_return"] = df_lbl["close"].shift(-1) / df_lbl["close"] - 1.0

    # –ß–∏—Å—Ç–∏–º –∏ —Ñ–∏–∫—Å–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å
    df_lbl = df_lbl.dropna(subset=["target", "fwd_return"]).reset_index(drop=True)

    # 3‚Äì4 —Å—Ü–µ–Ω–∞—Ä–∏—è —Ñ–∏—á (–≥—Ä–∞–¥–∏–µ–Ω—Ç –æ—Ç –ø—Ä–æ—Å—Ç–æ–≥–æ –∫ –ø–æ–ª–Ω–æ–º—É)
    feature_sets = {
        "LEVELS_ONLY": [
            "confluence_score",
            "sup_dist_atr",
            "res_dist_atr",
            "sup_strength",
            "res_strength",
            "level_quality",
        ],
        "LEVELS_PLUS_MOMENTUM": [
            "confluence_score",
            "sup_dist_atr",
            "res_dist_atr",
            "sup_strength",
            "res_strength",
            "level_quality",
            "rvol",
            "rsi",
            "adx",
            "volatility",
        ],
        "STANDARD_FEATURE_COLS": Config.FEATURE_COLS,
        "UNIVERSAL_FEATURE_COLS": Config.UNIVERSAL_FEATURE_COLS,
    }

    results = []
    for name, cols in feature_sets.items():
        res = evaluate_feature_set(name, cols, df_lbl)
        if res is not None:
            results.append(res)

    if not results:
        print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—á–∏—Ç–∞—Ç—å –Ω–∏ –æ–¥–∏–Ω —Å—Ü–µ–Ω–∞—Ä–∏–π.")
        return

    print("\n" + "=" * 60)
    print("üìä SUMMARY (sorted by Sharpe)")
    print("=" * 60)

    df_res = pd.DataFrame(results)
    df_res = df_res.sort_values("sharpe", ascending=False)
    with pd.option_context("display.max_columns", 10, "display.width", 120):
        print(df_res.to_string(index=False, float_format=lambda x: f"{x:0.4f}"))


if __name__ == "__main__":
    main()
