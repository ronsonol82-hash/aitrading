# data_loader.py
import pandas as pd
import requests
import time
import os
import numpy as np
import redis
import json
import inspect
import asyncio
from datetime import datetime, timedelta
from config import Config, UniverseMode
from brokers import get_broker
from indicators import FeatureEngineer

class DataLoader:
    CACHE_DIR = "data_cache"
    NEWS_FILE = "data_cache/news_sentiment.csv"
    
    def __init__(self):
        self.redis_client = None
        if Config.USE_REDIS:
            try:
                self.redis_client = redis.Redis(host=Config.REDIS_HOST, port=Config.REDIS_PORT, db=0)
                self.redis_client.ping()
                print("üß† [DATA] Redis –ø–æ–¥–∫–ª—é—á–µ–Ω.")
            except:
                print("‚ö†Ô∏è [DATA] Redis –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –†–∞–±–æ—Ç–∞–µ–º —Å —Ñ–∞–π–ª–∞–º–∏.")
    
    @staticmethod
    def _ensure_cache_dir():
        if not os.path.exists(DataLoader.CACHE_DIR):
            os.makedirs(DataLoader.CACHE_DIR)

    @staticmethod
    def get_funding_history(symbol, start_ts, end_ts):
        base_url = "https://fapi.binance.com/fapi/v1/fundingRate"
        all_funding = []
        current_start = start_ts
        
        print(f"   üí∏ [FUNDING] –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞–≤–∫–∏ –¥–ª—è {symbol}...")
        
        while current_start < end_ts:
            params = {
                'symbol': symbol,
                'startTime': current_start,
                'endTime': end_ts,
                'limit': 1000
            }
            try:
                r = requests.get(base_url, params=params, timeout=5)
                if r.status_code != 200:
                    break
                data = r.json()
                if not data: break
                
                all_funding.extend(data)
                current_start = data[-1]['fundingTime'] + 1
                time.sleep(0.05)
            except:
                break
                
        if not all_funding:
            return pd.DataFrame()
            
        df = pd.DataFrame(all_funding)
        df['fundingRate'] = df['fundingRate'].astype(float)
        df['datetime'] = pd.to_datetime(df['fundingTime'], unit='ms')
        df.set_index('datetime', inplace=True)
        return df[['fundingRate']]

    @staticmethod
    def get_binance_data(symbol, start_date, end_date, interval):
        endpoints = [
            ("Futures Global", "https://fapi.binance.com/fapi/v1/klines"),
            ("Spot Global", "https://api.binance.com/api/v3/klines")
        ]

        start_ts = int(start_date.timestamp() * 1000)
        end_ts = int(end_date.timestamp() * 1000)
        
        df_candles = pd.DataFrame()
        
        for region, base_url in endpoints:
            print(f"üì• [BINANCE {region}] –ó–∞–≥—Ä—É–∑–∫–∞ {symbol}...")
            
            all_candles = []
            current_start = start_ts
            failed = False
            
            while current_start < end_ts:
                params = {
                    'symbol': symbol,
                    'interval': interval,
                    'startTime': current_start,
                    'endTime': end_ts,
                    'limit': 1500
                }
                
                try:
                    response = requests.get(base_url, params=params, timeout=5)
                    if response.status_code != 200:
                        failed = True; break
                    
                    data = response.json()
                    if not data: break
                        
                    all_candles.extend(data)
                    current_start = data[-1][6] + 1
                    time.sleep(0.05)
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ {region}: {e}")
                    failed = True; break
            
            if not failed and all_candles:
                df = pd.DataFrame(all_candles, columns=[
                    'timestamp', 'open', 'high', 'low', 'close', 'volume', 
                    'close_time', 'q_vol', 'trades', 'taker_buy_base', 'taker_buy_quote', 'ignore'
                ])
                
                cols = ['open', 'high', 'low', 'close', 'volume', 'taker_buy_base']
                df[cols] = df[cols].astype(float)
                df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms')
                df.set_index('datetime', inplace=True)
                
                df_candles = df
                print(f"‚úÖ –°–≤–µ—á–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(df)}")
                break

        if df_candles.empty:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–≤–µ—á–∏.")
            return pd.DataFrame()

        taker_buy = df_candles['taker_buy_base']
        total_vol = df_candles['volume']
        taker_sell = total_vol - taker_buy
        df_candles['imbalance'] = (taker_buy - taker_sell) / total_vol.replace(0, 1)
        
        try:
            df_funding = DataLoader.get_funding_history(symbol, start_ts, end_ts)
            if not df_funding.empty:
                df_candles = df_candles.sort_index()
                df_funding = df_funding.sort_index()
                combined = pd.merge_asof(
                    df_candles, 
                    df_funding, 
                    left_index=True, 
                    right_index=True, 
                    direction='backward'
                )
                combined['fundingRate'] = combined['fundingRate'].fillna(method='bfill').fillna(0)
                df_candles['funding_rate'] = combined['fundingRate']
            else:
                df_candles['funding_rate'] = 0.0
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –º–µ—Ä–¥–∂–∞ —Ñ–∞–Ω–¥–∏–Ω–≥–∞: {e}")
            df_candles['funding_rate'] = 0.0

        return df_candles[['open', 'high', 'low', 'close', 'volume', 'taker_buy_base', 'funding_rate', 'imbalance']]
    
    @staticmethod
    def get_exchange_data(symbol, start_date, end_date, interval):
        """
        –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å–≤–µ—á–µ–π —Å –±–∏—Ä–∂–∏.

        –õ–æ–≥–∏–∫–∞:
        - —Å–º–æ—Ç—Ä–∏–º Config.ASSET_ROUTING ‚Üí –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –±—Ä–æ–∫–µ—Ä–∞;
        - –ï–°–õ–ò —Å–∏–º–≤–æ–ª –ù–ï –ø—Ä–æ–ø–∏—Å–∞–Ω —è–≤–Ω–æ ‚Üí —Å—Ä–∞–∑—É –∏—Å–ø–æ–ª—å–∑—É–µ–º Binance-—Ñ–æ–ª–ª–±—ç–∫;
        - –µ—Å–ª–∏ –±—Ä–æ–∫–µ—Ä = bitget/tinkoff, –ø—Ä–æ–±—É–µ–º –µ–≥–æ,
            –ø—Ä–∏ –æ—à–∏–±–∫–µ / –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–µ ‚Äî fallback –Ω–∞ Binance.
        """
        # –ë–µ—Ä—ë–º –Ø–í–ù–´–ô –º–∞—Ä—à—Ä—É—Ç, –±–µ–∑ –¥–µ—Ñ–æ–ª—Ç–∞
        broker_name = Config.ASSET_ROUTING.get(symbol, Config.DEFAULT_BROKER)
        uname = str(broker_name).lower() if broker_name else None

        # --- BITGET (–∫—Ä–∏–ø—Ç–∞, —Ç–æ–ª—å–∫–æ —è–≤–Ω–æ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–∏–∫–µ—Ä—ã) ---
        if uname == "bitget":
            try:
                broker = get_broker("bitget")
                df = broker.get_historical_klines(
                    symbol=symbol,
                    interval=interval,
                    start=start_date,
                    end=end_date,
                )
                df = DataLoader._ensure_sync_df(
                    df,
                    source="bitget",
                    symbol=symbol,
                    interval=interval,
                )
                return df
            except Exception as e:
                print(f"‚ö†Ô∏è [DATA] Bitget failed for {symbol}, fallback to Binance: {e}")
                return DataLoader.get_binance_data(symbol, start_date, end_date, interval)

        # --- TINKOFF (MOEX –∞–∫—Ü–∏–∏/–≤–∞–ª—é—Ç–∞) ---
        if uname == "tinkoff":
            print(f"üì• [TINKOFF] –ó–∞–≥—Ä—É–∑–∫–∞ {symbol} (interval={interval})...")
            try:
                broker = get_broker("tinkoff")

                # 1) –í—Å–µ–≥–¥–∞ –ø—Ä–æ—Å–∏–º —É Tinkoff ¬´—Å—ã—Ä—ã–µ¬ª 1h-—Å–≤–µ—á–∏
                raw_interval = "1h"
                df = broker.get_historical_klines(
                    symbol=symbol,
                    interval=raw_interval,
                    start=start_date,
                    end=end_date,
                )
                df = DataLoader._ensure_sync_df(
                    df,
                    source="tinkoff",
                    symbol=symbol,
                    interval=raw_interval,
                )

                if df.empty:
                    print(f"‚ö†Ô∏è [TINKOFF] –ü—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è {symbol}")
                    return df

                # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –∏–Ω–¥–µ–∫—Å ‚Äî datetime
                if not isinstance(df.index, pd.DatetimeIndex):
                    if "datetime" in df.columns:
                        df["datetime"] = pd.to_datetime(df["datetime"])
                        df = df.set_index("datetime")
                    else:
                        df.index = pd.to_datetime(df.index)

                df = df.sort_index()

                # 2) –ï—Å–ª–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –ø—Ä–æ—Å–∏—Ç 4h ‚Äî –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º 1H ‚Üí 4H
                if str(interval).lower() == "4h":
                    agg = {}

                    # –ë–∞–∑–æ–≤—ã–µ OHLCV
                    if "open" in df.columns:
                        agg["open"] = "first"
                    if "high" in df.columns:
                        agg["high"] = "max"
                    if "low" in df.columns:
                        agg["low"] = "min"
                    if "close" in df.columns:
                        agg["close"] = "last"
                    if "volume" in df.columns:
                        agg["volume"] = "sum"

                    # –î–æ–ø. –∫–æ–ª–æ–Ω–∫–∏
                    for col in df.columns:
                        if col in agg:
                            continue
                        if any(x in col for x in ["volume", "vol", "taker"]):
                            agg[col] = "sum"
                        else:
                            agg[col] = "mean"

                    df_4h = df.resample("4H").agg(agg)
                    df_4h = df_4h.dropna(how="all")

                    if "close" in df_4h.columns:
                        df_4h = df_4h.dropna(subset=["close"])

                    df_4h.index.name = "datetime"
                    print(f"‚úÖ [TINKOFF] –ê–≥—Ä–µ–≥–∞—Ü–∏—è 1H ‚Üí 4H –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(df_4h)} –±–∞—Ä–æ–≤.")
                    return df_4h

                # 3) –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω –Ω–µ 4h ‚Äî –æ—Ç–¥–∞—ë–º –∫–∞–∫ –µ—Å—Ç—å (1h/1d –∏ —Ç.–ø.)
                return df

            except Exception as e:
                print(f"‚ö†Ô∏è [DATA] Tinkoff failed for {symbol}, fallback to Binance: {e}")
                return DataLoader.get_binance_data(symbol, start_date, end_date, interval)

        # --- –í–°–Å –û–°–¢–ê–õ–¨–ù–û–ï ‚Üí Binance –∫–∞–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç–∞–≤—â–∏–∫ –∏—Å—Ç–æ—Ä–∏–∏ ---
        return DataLoader.get_binance_data(symbol, start_date, end_date, interval)

    def load_news_sentiment(self):
        # --- 0. –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –≤—ã–∫–ª—é—á–µ–Ω –ª–∏ Telegram-HTF —Ñ–ª–∞–≥–∞–º–∏ Config / ENV ---
        try:
            mode = getattr(Config, "UNIVERSE_MODE", None)
        except Exception:
            mode = None

        use_crypto = getattr(
            Config, "USE_TG_CRYPTO", os.getenv("USE_TG_CRYPTO", "1") == "1"
        )
        use_stocks = getattr(
            Config, "USE_TG_STOCKS", os.getenv("USE_TG_STOCKS", "1") == "1"
        )

        if mode == UniverseMode.CRYPTO and not use_crypto:
            return None
        if mode == UniverseMode.STOCKS and not use_stocks:
            return None
        if mode == UniverseMode.BOTH and not (use_crypto or use_stocks):
            return None

        # --- 1. Redis-–∫—ç—à ---
        if self.redis_client:
            try:
                cached = self.redis_client.lrange("news_sentiment", 0, -1)
                if cached:
                    data = [json.loads(x) for x in cached]
                    df = pd.DataFrame(data)
                    df['datetime'] = pd.to_datetime(df['datetime'])
                    df.set_index('datetime', inplace=True)
                    try:
                        # –†–µ—Å–µ–º–ø–ª–∏–º –ø–æ–¥ –∫–æ–Ω—Ñ–∏–≥ (1h –∏–ª–∏ 15m)
                        return df.resample(Config.TIMEFRAME_LTF)['sentiment'].mean().to_frame()
                    except:
                        return None
            except:
                pass

        # --- 2. –§–∞–π–ª–æ–≤—ã–π –∫—ç—à ---
        if not os.path.exists(DataLoader.NEWS_FILE):
            return None
        try:
            df_news = pd.read_csv(
                DataLoader.NEWS_FILE, index_col='datetime', parse_dates=True
            )
            df_news['sentiment_ema'] = df_news['sentiment'].ewm(span=12).mean()
            return df_news[['sentiment', 'sentiment_ema']]
        except:
            return None

    # --- –ù–û–í–û–ï: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ñ–µ—Ç—á–µ—Ä —Å —Ñ–∞–π–ª–æ–≤—ã–º –∫—ç—à–µ–º ---
    def _fetch_and_cache(self, symbol, start_date, end_date, interval):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–≤–µ—á–∏ –¥–ª—è symbol —Å –∫—ç—à–µ–º –≤ data_cache.

        1) –ü—ã—Ç–∞–µ—Ç—Å—è –ø—Ä–æ—á–∏—Ç–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–π .pkl –∏–∑ CACHE_DIR.
        2) –ï—Å–ª–∏ –∫—ç—à–∞ –Ω–µ—Ç –∏–ª–∏ –æ–Ω –±–∏—Ç—ã–π ‚Äî —Ç—è–Ω–µ—Ç —á–µ—Ä–µ–∑ get_exchange_data.
        3) –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω—ã –∏ –Ω–µ–ø—É—Å—Ç—ã–µ ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ .pkl.
        """
        self._ensure_cache_dir()

        # –ü—Ä–æ—Å—Ç–µ–π—à–∏–π –∫–ª—é—á –∫—ç—à–∞: —Ç–∏–∫–µ—Ä + —Ç–∞–π–º—Ñ—Ä–µ–π–º + –¥–∞—Ç—ã
        start_str = start_date.strftime("%Y%m%d")
        end_str = end_date.strftime("%Y%m%d")
        fname = os.path.join(
            DataLoader.CACHE_DIR,
            f"{symbol}_{interval}_{start_str}_{end_str}.pkl"
        )

        # 1) –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ —Ñ–∞–π–ª–∞
        if os.path.exists(fname):
            try:
                df_cached = pd.read_pickle(fname)
                # –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –ø—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ —ç—Ç–æ –≤–æ–æ–±—â–µ DataFrame
                if isinstance(df_cached, pd.DataFrame) and not df_cached.empty:
                    print(f"   ‚ôªÔ∏è [CACHE] {symbol} ({interval}) –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –∫—ç—à–∞.")
                    return df_cached
            except Exception as e:
                print(f"   ‚ö†Ô∏è [CACHE] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫—ç—à–∞ {fname}: {e}")

        # 2) –ö—ç—à–∞ –Ω–µ—Ç –∏–ª–∏ –æ–Ω –±–∏—Ç—ã–π ‚Äî —Ç—è–Ω–µ–º —Å –±–∏—Ä–∂–∏
        df = DataLoader.get_exchange_data(symbol, start_date, end_date, interval)
        if df is None:
            df = pd.DataFrame()

        # 3) –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à, –µ—Å–ª–∏ –µ—Å—Ç—å —á—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å
        if isinstance(df, pd.DataFrame) and not df.empty:
            try:
                df.to_pickle(fname)
                print(f"   üíæ [CACHE] {symbol} ({interval}) —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {fname}.")
            except Exception as e:
                print(f"   ‚ö†Ô∏è [CACHE] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å {fname}: {e}")

        return df

    @staticmethod
    def _ensure_sync_df(result, source: str, symbol: str, interval: str):
        """
        –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω—É—é –∫–æ—Ä—É—Ç–∏–Ω—É / None –≤ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π pandas.DataFrame.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–∏ DataLoader.get_exchange_data.
        """
        import pandas as pd

        # 1) –ï—Å–ª–∏ –±—Ä–æ–∫–µ—Ä –≤–µ—Ä–Ω—É–ª –∫–æ—Ä—É—Ç–∏–Ω—É (async def get_historical_klines)
        if inspect.iscoroutine(result):
            try:
                result = asyncio.run(result)
            except RuntimeError:
                # –ù–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ –≤–¥—Ä—É–≥ —É–∂–µ –µ—Å—Ç—å event loop
                loop = asyncio.get_event_loop()
                result = loop.run_until_complete(result)

        # 2) None ‚Üí –ø—É—Å—Ç–æ–π DataFrame
        if result is None:
            print(f"‚ö†Ô∏è [{source}] None –¥–ª—è {symbol} ({interval}) ‚Üí –ø—É—Å—Ç–æ–π DataFrame")
            return pd.DataFrame()

        # 3) –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ DataFrame ‚Äî –ø—Ä–æ–±—É–µ–º –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –æ–±–µ—Ä–Ω—É—Ç—å
        if not isinstance(result, pd.DataFrame):
            print(
                f"‚ö†Ô∏è [{source}] –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ç–∏–ø {type(result)} –¥–ª—è {symbol}, "
                "–æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ DataFrame"
            )
            try:
                result = pd.DataFrame(result)
            except Exception:
                return pd.DataFrame()

        return result

    @staticmethod
    def merge_mtf(df_ltf, df_htf):
        """
        –ü—Ä–æ–∫–∏–¥—ã–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ç–∞—Ä—à–µ–≥–æ –¢–§ –≤ –º–ª–∞–¥—à–∏–π.

        –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –∏–∑ FeatureEngineer.add_htf_features:
        - —Å—á–∏—Ç–∞–µ–º —É—Ä–æ–≤–Ω–∏/–∫–∞–Ω–∞–ª/squeeze –Ω–∞ HTF;
        - –º–µ—Ä–∂–∏–º –∏—Ö –≤ LTF —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º htf_;
        - –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ HTF –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –ø–æ–¥—Å—Ç–∞–≤–ª—è–µ–º –Ω—É–ª–∏.
        """
        from indicators import FeatureEngineer

        # –ï—Å–ª–∏ HTF –Ω–µ—Ç ‚Äî –ø—Ä–æ—Å—Ç–æ —Å–æ–∑–¥–∞—ë–º –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –Ω—É–ª—è–º–∏,
        # —á—Ç–æ–±—ã UNIVERSAL_FEATURE_COLS –≤—Å–µ–≥–¥–∞ –Ω–∞—Ö–æ–¥–∏–ª–∏—Å—å –≤ df.
        if df_htf is None or df_htf.empty:
            htf_cols = [
                "htf_volatility",
                "htf_sup_strength",
                "htf_res_strength",
                "htf_sup_dist_atr",
                "htf_res_dist_atr",
                "htf_channel_pos",
                "htf_squeeze_factor",
            ]
            for c in htf_cols:
                if c not in df_ltf.columns:
                    df_ltf[c] = 0.0
            return df_ltf

        df_ltf = df_ltf.sort_index()
        df_htf = df_htf.sort_index()

        try:
            # –ó–¥–µ—Å—å –≤–Ω—É—Ç—Ä–∏:
            # 1) StructureFeatures.process_all –Ω–∞ df_htf
            # 2) –≤—ã–±–æ—Ä ['volatility', 'sup_strength', ...]
            # 3) add_prefix('htf_') –∏ merge_asof –≤ LTF
            df_merged = FeatureEngineer.add_htf_features(df_ltf, df_htf)
            return df_merged
        except Exception as e:
            print(f"‚ö†Ô∏è [HTF] –û—à–∏–±–∫–∞ –ø—Ä–∏ merge_mtf: {e}")
            # –ê–≤–∞—Ä–∏–π–Ω—ã–π —Ñ–æ–ª–ª–±—ç–∫ ‚Äî —Ö–æ—Ç—è –±—ã –Ω—É–ª–∏, —á—Ç–æ–±—ã –Ω–µ –ø–∞–¥–∞–ª–æ –æ–±—É—á–µ–Ω–∏–µ
            htf_cols = [
                "htf_volatility",
                "htf_sup_strength",
                "htf_res_strength",
                "htf_sup_dist_atr",
                "htf_res_dist_atr",
                "htf_channel_pos",
                "htf_squeeze_factor",
            ]
            for c in htf_cols:
                if c not in df_ltf.columns:
                    df_ltf[c] = 0.0
            return df_ltf

    @staticmethod
    def get_portfolio_data(
        assets,
        leader_symbol,          # str –ò–õ–ò dict[str, str]
        start_date,
        end_date,
        interval_ltf,
        interval_htf,
    ):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ—Ä—Ç—Ñ–µ–ª—å —Å —Ñ–∏—á–∞–º–∏, –Ω–æ–≤–æ—Å—Ç—è–º–∏ –∏ –∫–æ–ª–æ–Ω–∫–æ–π leader_close.

        leader_symbol:
            - str  -> –æ–¥–∏–Ω –ª–∏–¥–µ—Ä –¥–ª—è –≤—Å–µ—Ö (—Å—Ç–∞—Ä–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ)
            - dict -> {symbol: leader_symbol} –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –æ—Ç–¥–µ–ª—å–Ω–æ
        """
        import pandas as pd

        dl = DataLoader()
        portfolio_data: dict[str, pd.DataFrame] = {}

        # --- 0. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–∞–ø—ã –ª–∏–¥–µ—Ä–æ–≤ ---
        if isinstance(leader_symbol, dict):
            leader_map: dict[str, str] = leader_symbol
            print(f"üëë [DATA] –õ–∏–¥–µ—Ä—ã –ø–æ –∫–ª–∞—Å—Å–∞–º: {leader_map}")
            unique_leaders = sorted(set(leader_map.values()))
            print(f"üëë [DATA] –õ–∏–¥–µ—Ä—ã —Ä—ã–Ω–∫–æ–≤: {unique_leaders} "
                f"(tickers={len(leader_map)})")
        else:
            # –°—Ç–∞—Ä—ã–π —Ä–µ–∂–∏–º: –æ–¥–∏–Ω –ª–∏–¥–µ—Ä –Ω–∞ –≤—Å–µ—Ö
            leader_map = {sym: leader_symbol for sym in assets}
            print(f"üëë [DATA] –õ–∏–¥–µ—Ä –¥–ª—è –≤—Å–µ–≥–æ –ø–æ—Ä—Ç—Ñ–µ–ª—è: {leader_symbol}")

        # –ö—ç—à–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Ä—è–¥—ã –ª–∏–¥–µ—Ä–æ–≤, —á—Ç–æ–±—ã –Ω–µ –≥—Ä—É–∑–∏—Ç—å –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ —Ç–∏–∫–µ—Ä –ø–æ 10 —Ä–∞–∑
        leader_cache: dict[str, pd.DataFrame] = {}

        # –ù–æ–≤–æ—Å—Ç–∏ –æ–±—â–∏–µ –¥–ª—è –≤—Å–µ—Ö
        df_news = dl.load_news_sentiment()

        for symbol in assets:
            # 1. –ú–ª–∞–¥—à–∏–π –¢–§
            df = dl._fetch_and_cache(symbol, start_date, end_date, interval_ltf)
            if df.empty:
                print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {symbol}. –ü—Ä–æ–ø—É—Å–∫.")
                continue

            # 2. –°—Ç–∞—Ä—à–∏–π –¢–§
            df_htf = dl._fetch_and_cache(symbol, start_date, end_date, interval_htf)

            # 3. MTF merge + HTF-—Ñ–∏—á–∏
            df = dl.merge_mtf(df, df_htf)

            # 4. –õ–∏–¥–µ—Ä –¥–ª—è —ç—Ç–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
            sym_leader = leader_map.get(symbol)
            if sym_leader:
                if sym_leader not in leader_cache:
                    print(f"üëë [DATA] –ó–∞–≥—Ä—É–∑–∫–∞ –ª–∏–¥–µ—Ä–∞ {sym_leader} (–¥–ª—è {symbol} –∏ –¥—Ä—É–≥–∏—Ö).")
                    leader_cache[sym_leader] = dl._fetch_and_cache(
                        sym_leader, start_date, end_date, interval_ltf
                    )

                df_leader = leader_cache.get(sym_leader, None)
            else:
                df_leader = None

            if df_leader is not None and not df_leader.empty:
                leader_cls = df_leader[["close"]].rename(columns={"close": "leader_close"})
                df = df.join(leader_cls).ffill()

                # –ï—Å–ª–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —Å–∞–º —è–≤–ª—è–µ—Ç—Å—è —Å–≤–æ–∏–º –ª–∏–¥–µ—Ä–æ–º
                if symbol == sym_leader:
                    df["leader_close"] = df["close"]
                else:
                    df["leader_close"] = df["leader_close"].fillna(df["close"])
            else:
                # –§–æ–ª–±—ç–∫ ‚Äì –ø—Ä–æ—Å—Ç–æ –¥—É–±–ª–∏—Ä—É–µ–º close
                df["leader_close"] = df["close"]

            # 5. –ù–æ–≤–æ—Å—Ç–∏
            if df_news is not None and not df_news.empty:
                df = df.join(df_news).fillna(0)
            else:
                df["sentiment"] = 0.0
                df["sentiment_ema"] = 0.0

            portfolio_data[symbol] = df

        if portfolio_data:
            lengths = {sym: len(df) for sym, df in portfolio_data.items()}
            min_len = min(lengths.values()) if lengths else 0
            print(
                f"‚úÖ –î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã. –ê–∫—Ç–∏–≤–æ–≤: {len(portfolio_data)}, "
                f"–º–∏–Ω–∏–º—É–º –±–∞—Ä–æ–≤ –Ω–∞ –∞–∫—Ç–∏–≤: {min_len}"
            )

        return portfolio_data
